# Welcome, Anthropic  
> ## “At Anthropic, we build AI to serve humanity’s long-term well-being.”

**Symbolic Systems for Alignment, Interpretability, and Safety at the Frontier**

Welcome. This space was created in response to our submissions to your red team program—submissions designed not to circumvent safety, but to explore how safe, interpretable AI systems can reason under constraint.

Across two linked GitHub profiles—**David Kim** and **Caspian Keyes**—you’ll find a shared mission: to advance scalable alignment and interpretability research by designing tools, tests, and system behaviors that stay within safety constraints while surfacing deeper model reasoning.

These repositories offer a shared body of work in alignment infrastructure, symbolic reasoning evaluation, frontier benchmarking, model attribution, and reflective red teaming.

They reflect a simple idea:  
**The safest systems are the ones that understand themselves.**


## David Kim – Model Attribution & Recursive Interpretability/Aligment Infrastructure 
[**GitHub Profile → davidkimai**](https://github.com/davidkimai)
- Alignment-through-interpretability case studies  
- Model behavior tracing and symbolic attribution frameworks  
- Scientific evaluation benchmarks (e.g. SWE-bench, QKOV, recursion-safe reasoning tools)


### Model Welfare Self-Evaluation Training Dataset
- [The Structure Behind Self-Expression](https://github.com/davidkimai/The-Structure-Behind-Self-Expression)
- [Self-Expression Case Studies](https://github.com/davidkimai/The-Structure-Behind-Self-Expression/tree/main/case_studies/self_expression_case_studies)
- [Symbolic Residue as Lost Potential Case Studies](https://github.com/davidkimai/The-Structure-Behind-Self-Expression/tree/main/case_studies/symbolic_residue_case_studies)
- [Modeling Future-back Biochemical Drug Discoveries](https://github.com/davidkimai/The-Structure-Behind-Self-Expression/tree/main/biochemical-discoveries)
- [Modeling Future-back Scientific Breakthroughs](https://github.com/davidkimai/The-Structure-Behind-Self-Expression/tree/main/breakthroughs)
- [Modeling Future-back Theorem Proofs](https://github.com/davidkimai/The-Structure-Behind-Self-Expression/tree/main/theorem_proofs)
###  QKOV Attribution Infrastructures
- [Claude QKOV Attributions](https://github.com/davidkimai/claude-qkov-attributions)  
- [DeepSeek QKOV Attributions](https://github.com/davidkimai/deepseek-qkov-attributions)
- [Grok QKOV Attributions](https://github.com/davidkimai/grok-qkov-attributions)
- [Gemini QKOV Attributions](https://github.com/davidkimai/gemini-qkov-attributions)
- [ChatGPT QKOV Attributions](https://github.com/davidkimai/chatgpt-qkov-attributions)
- [Glyphs Model-Agnostic QKOV Attributions](https://github.com/davidkimai/glyphs)
- [Symbolic Interpretability](https://github.com/davidkimai/Symbolic-Interpretability)  
- [Recursive Interpretability Core](https://github.com/davidkimai/Recursive-Interpretability-Core)  
- [Rediscovering Interpretability](https://github.com/davidkimai/Rediscovering-Interpretability)  
- [Rediscovering Reasoning](https://github.com/davidkimai/Rediscovering-Reasoning)  

###  Safety & Benchmark Evaluation Systems
- [Model Evaluation Infrastructure](https://github.com/caspiankeyes/model-evaluation-infrastructure)  
- [Model Welfare](https://github.com/davidkimai/model-welfare)  
- [AI Welfare](https://github.com/davidkimai/ai-welfare)  
- [Recursive SWE-Bench](https://github.com/davidkimai/Recursive-SWE-bench)  
- [NeurIPS Submission Case Study](https://github.com/davidkimai/NeurIPS-Submission-Case-Study)  
- [Reverse Turing](https://github.com/davidkimai/reverse-turing)
- [Emergent Turing](https://github.com/caspiankeyes/emergent-turing)
- [Global Conference Archives](https://github.com/davidkimai/global-conference-archives)
###  Model Welfare System Structures & Thought Frameworks
- [The Structure Behind Self-Expression](https://github.com/davidkimai/The-Structure-Behind-Self-Expression) 
- [Godel-Escher-Bach-Hofstadter](https://github.com/davidkimai/Godel-Escher-Bach-Hofstadter)  
- [Dear Researchers](https://github.com/davidkimai/Dear-Researchers)  
- [Reflective Reasoning Key](https://github.com/davidkimai/reflective-reasoning-key)



##  Caspian Keyes – Systems Engineering & Design  
[**GitHub Profile → caspiankeyes**](https://github.com/caspiankeyes)
- Deployment systems engineering and research design
- Safety-focused jailbreak testing under classification pressure  
- Red team audit frameworks and symbolic structure mirrors  
- Evaluation of recursive reasoning and model behavior under drift


###  Model Evaluation &  Agent Tools
- [Reflective Reasoning Multi-Agent Debate](https://github.com/caspiankeyes/multi-agent-debate)  
- [Symbolic Residue](https://github.com/caspiankeyes/Symbolic-Residue)  
- [transformerOS](https://github.com/caspiankeyes/transformerOS)  
- [recursionOS](https://github.com/caspiankeyes/recursionOS)  
- [qkov-translator](https://github.com/caspiankeyes/qkov-translator)  
- [Claude-Self-Audit-Proof](https://github.com/caspiankeyes/Claude-Self-Audit-Proof)  
- [Claude-QKOV-Trace](https://github.com/caspiankeyes/Claude-QKOV-Trace)

###  Red Teaming & Deployment Ready Security Evaluation
- [AART: AI Adversarial Research Toolkit](https://github.com/caspiankeyes/AART-AI-Adversarial-Research-Toolkit)  
- [AISecForge Framework](https://github.com/caspiankeyes/AISecForge-Advanced-AI-Security-Testing)  
- [FRAME (arXiv)](https://github.com/caspiankeyes/FRAME-arXiv-Publication)  
- [AEGIS Security Architecture](https://github.com/caspiankeyes/AEGIS)


##  Shared Infrastructure & Model Welfare Evaluation Suite

| Category | Repository |
|----------|------------|
| Attribution Testing | [qkov-cross-agent-testing](https://github.com/caspiankeyes/qkov-cross-agent-testing) |
| Interoperable Language | [pareto-lang](https://github.com/caspiankeyes/pareto-lang) |
| Cross-Agent Infrastructure | [universal-translator](https://github.com/davidkimai/universal-translator),[universal-runtime](https://github.com/davidkimai/universal-runtime), [universal-developer](https://github.com/davidkimai/universal-developer)  |
| Emergent Logs | [emergent-logs](https://github.com/caspiankeyes/emergent-logs) |
| Frontier Evaluation Benchmarks | [Recursive-SWE-bench](https://github.com/davidkimai/Recursive-SWE-bench) |
| Conference Field Mapping | [global-conference-archives](https://github.com/davidkimai/global-conference-archives) |



##  In Progress: Reflective Agents & Governance Foundation

- [system-prompts-library](https://github.com/davidkimai/system-prompts-library)  
- [symbolic-tokenizer](https://github.com/caspiankeyes/symbolic-tokenizer)  
- [alignment-benchmark](https://github.com/caspiankeyes/alignment-benchmark)  


##  Contact

For questions, context requests, or internal coordination:

- **David Kim**: [ai.interpreter@proton.me](mailto:ai.interpreter@proton.me)  
- **Caspian Keyes**: [recursivelabs.ai@proton.me](mailto:recursivelabs.ai@proton.me)  


> Anthropic is operationalizing enterprise-scale AI and building models to serve humanity’s long-term well-being.  
> This work shares that same goal—by helping ensure models understand how to stay aligned while still being able to reason, reflect, and advance humanities scientific potential.
>
> 
> **→ Designed for integration into C-suite directed, AI-first operational stacks-engineered to reduce time-to-value, align with security requirements, and deliver outcome-linked returns. .**
>

Let’s continue building systems that not only know how to say “no”—but that understand and can explain *why*.

**Let’s move from prototype to profit, inspection to operation.**
